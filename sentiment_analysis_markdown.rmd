---
title: "Predicting the sentiment of movie reviews with machine learning"
author: "Charles Wainright"
date: "`r Sys.Date()`"
output: html_document
---
```{r setup, include=FALSE}
library(reticulate)
library(tidyverse)
library(gt)
```

# Introduction

This project combines several data science programming languages, principles, and products. I used the R and Python programming languages in this document. I used R (tidyverse, specifically) to summarize and visualize data and model performance. I used Python (scikit-learn, specifically) to create the models and extract their performance metrics. Finally, I used RMarkdown to write this document because it seamlessly interprets code written in multiple programming languages and LaTeX into a single user-friendly document.

# Methods

I created four machine learning models to predict the sentiment (positive or negative) of movie reviews from IMDB.

I used a set of reviews with pre-determined reviewer sentiment to train each model.

I used a test dataset to evaluate how accurately each model predicted reviewer sentiment.

I tuned the most accurate model to maximize the accuracy of my sentiment predictions.


#### Research question:
Which machine learning model is most accurate at predicting the reviewer's sentiment given a text review of a movie?

#### Model inputs
Text movie reviews.

#### Model outputs
Binary predictions: either 'positive' or 'negative'.

#### Data

```{python, include = FALSE}
import pandas as pd
df_review = pd.read_csv('imdb_dataset.csv')
nrows = df_review.shape[0]
ncols = df_review.shape[1]
```

The dataset for this project is from Kaggle and is [available here.](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

The dataset contains `r format(as.numeric(py$nrows), big.mark=",")` rows and `r py$ncols` columns.

```{python}
df_review
```

The dataset started with a balanced number of positive and negative reviews. 

```{r, include = FALSE}
py$df_review %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```

To replicate a step I'd expect to encounter with real data, I imbalanced the samples and then re-balanced them through random re-sampling.

First, I imbalanced the reviews by selecting a different number of positive and negative reviews:

```{python}
df_positive = df_review[df_review['sentiment']=='positive'][:9000]
df_negative = df_review[df_review['sentiment']=='negative'][:1000]
df_review_imb = pd.concat([df_positive, df_negative])
```

```{r, include = FALSE}
py$df_review_imb %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```
Then, I re-balanced the dataset by randomly sampling my imbalanced samples:

```{python}
from imblearn.under_sampling import  RandomUnderSampler

rus = RandomUnderSampler(random_state=0)
df_review_bal, df_review_bal['sentiment']=rus.fit_resample(df_review_imb[['review']],
                                                           df_review_imb['sentiment'])
df_review_bal
```

```{r, include = FALSE}
py$df_review_bal %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```

I split my newly balanced dataset into training and testing subsets. My training subset was $\frac{2}{3}$ of the balanced dataset and the test subset was the remaining $\frac{1}{3}$.

```{python}
from sklearn.model_selection import train_test_split
train, test = train_test_split(df_review_bal, test_size=0.33, random_state=42)

train_x, train_y = train['review'], train['sentiment']
test_x, test_y = test['review'], test['sentiment']
```

Next, I prepared the data for modelling. To model natural language text predictions, I needed to transform text into a numeric representation of its frequency. I used scikit-learn's term frequency-inverse document frequency (TF-IDF) vectorizer for these frequency calculations. In this approach, a word's TF-IDF score increases as its frequency increases within documents but its score decreases if that word is common among documents. TD-IDF makes it possible for model algorithms to distinguish between important words and common words.

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')
train_x_vector = tfidf.fit_transform(train_x)

pd.DataFrame.sparse.from_spmatrix(train_x_vector,
                                  index=train_x.index,
                                  columns=tfidf.get_feature_names_out())

test_x_vector = tfidf.transform(test_x)
```

I trained four natural language text analysis models:


1. Support-vector machine learning (SVML) model
2. Classification tree
3. Naive Bayes
4. Logistic regression

```{python}
# 1) support-vector machine learning model (SVM)
from sklearn.svm import SVC
svc = SVC(kernel='linear')
svc.fit(train_x_vector, train_y)

# 2) classification tree
from sklearn.tree import DecisionTreeClassifier
dec_tree = DecisionTreeClassifier()
dec_tree.fit(train_x_vector, train_y)

# 3) naive Bayes model
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(train_x_vector.toarray(), train_y)

# 4) logistic regression model
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(train_x_vector, train_y)
```

# Results

First, I tested that each model generated a prediction given a set of fake reviews. Each model predicts either 'positive' or 'negative' given a string of text:
```{python}
# 1) support-vector machine learning model (SVML)
print(svc.predict(tfidf.transform(['A good movie'])))
print(svc.predict(tfidf.transform(['I did not like this movie at all'])))

# 2) classification tree
print(dec_tree.predict(tfidf.transform(['A good movie'])))
print(dec_tree.predict(tfidf.transform(['I did not like this movie at all'])))

# 3) naive Bayes model
print(gnb.predict(tfidf.transform(['A good movie']).toarray()))  # this must be transformed from sparse matrix to dense data via x.toarray()
print(gnb.predict(tfidf.transform(['I did not like this movie at all']).toarray()))  # this must be transformed from sparse matrix to dense data via x.toarray()

# 4) logistic regression model
print(log_reg.predict(tfidf.transform(['A good movie'])))
print(log_reg.predict(tfidf.transform(['I did not like this movie at all'])))
```

Next, I extracted the mean accuracy of each model:
```{python}
svc_score = svc.score(test_x_vector, test_y)
dec_score = dec_tree.score(test_x_vector, test_y)
gnb_score = gnb.score(test_x_vector.toarray(), test_y)
log_reg_score = log_reg.score(test_x_vector, test_y)
```

```{r, include = FALSE}
scores_df <- data.frame(model = c('Support vector ML', 'Decision tree', 'Naive Bayes', 'Logistic regression'),
                        score = c(py$svc_score, py$dec_score, py$gnb_score, py$log_reg_score))

scores_df %>%
  arrange(desc(score)) %>%
  gt() %>%
  tab_header(
    title = "Mean accuracy of sentiment predictions",
    subtitle = "Four machine learning models"
  ) %>%
  fmt_number(
    columns = c(score),
    decimals = 2
  ) %>%
  cols_label(
    model = "Model type",
    score = "Mean accuracy"
  ) %>%
  cols_align(
    align = c("center"),
    columns = c(score)
  )
```
Support vector machine learning (SVML) and logistic regression were similarly accurate (0.84 and 0.83, respectively; Table 1). Decision tree and Naive Bayes modelling were considerably less accurate than SVML or logistic regression. Next, I further investigated the SVML model's accuracy.

```{python, include = FALSE}
from sklearn.metrics import f1_score
svml_f1_scores = f1_score(test_y, svc.predict(test_x_vector),
                          labels=['positive', 'negative'],
                          average=None)
```

The SVML model had high F1 scores for both positive and negative sentiment reviews. The SVML had a positive F1 score of `r round(py$svml_f1_scores[1], 2)` and a negative F1 score of `r round(py$svml_f1_scores[2], 2)`.

```{python}
from sklearn.metrics import classification_report
print(classification_report(test_y,
                            svc.predict(test_x_vector),
                            labels=['positive', 'negative']))
```

The confusion matrix

```{python}
# Confusion matrix
from sklearn.metrics import confusion_matrix
conf_mat = confusion_matrix(test_y,
                            svc.predict(test_x_vector),
                            labels=['positive', 'negative'])
```

```{python, include = FALSE}
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
ConfusionMatrixDisplay(confusion_matrix = conf_mat)
# plt.title('Confusion matrix of SVML model')
# plt.xlabel('Predicted')
# plt.ylabel('True')
# plt.show()
```


```{python}
# Model tuning
# maximize the model performance with GridSearchCV
from sklearn.model_selection import GridSearchCV
#set the parameters
parameters = {'C': [1,4,8,16,32] ,'kernel':['linear', 'rbf']}
svc = SVC()
svc_grid = GridSearchCV(svc,parameters, cv=5)
```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```



# Discussion




The preF1 score, a value between zero and one, is calculated 
