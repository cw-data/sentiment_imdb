---
title: "Predicting the sentiment of movie reviews with machine learning"
author: "Charles Wainright"
date: "`r format(Sys.Date(), '%m %b, %Y')`"
output: html_document
---
```{r, warning = FALSE, message = FALSE}
library(reticulate)
library(tidyverse)
library(gt)
library(cvms)
set.seed(1)
```

```{python}
import pandas as pd
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
```

# Introduction

This project combines several data science programming languages, principles, and products. I used the R and Python programming languages in this document. I used R (tidyverse, specifically) to summarize and visualize data and model performance. I used Python (scikit-learn, specifically) to create the models and extract their performance metrics. Finally, I used RMarkdown to write this document because it seamlessly interprets code written in multiple programming languages and LaTeX into a single user-friendly document.

# Methods

I created four machine learning models to predict the sentiment (positive or negative) of movie reviews from IMDB.

I used a set of reviews with pre-determined reviewer sentiment to train each model.

I used a test dataset to evaluate how accurately each model predicted reviewer sentiment.

I tuned the most accurate model to maximize the accuracy of my sentiment predictions.


#### Research question:
Which machine learning model is most accurate at predicting the reviewer's sentiment given a text review of a movie?

#### Model inputs
Text movie reviews.

#### Model outputs
Binary predictions: either 'positive' or 'negative'.

#### Data

```{python, include = FALSE}
df_review = pd.read_csv('imdb_dataset.csv')
nrows = df_review.shape[0]
ncols = df_review.shape[1]
```

The dataset for this project is from Kaggle and is [available here.](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

The dataset contains `r format(as.numeric(py$nrows), big.mark=",")` rows and `r py$ncols` columns.

```{python}
df_review
```

The dataset started with a balanced number of positive and negative reviews. 

```{r, include = FALSE}
py$df_review %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```

To replicate a step I'd expect to encounter with real data, I imbalanced the samples and then re-balanced them through random re-sampling.

First, I imbalanced the reviews by selecting a different number of positive and negative reviews:

```{python}
df_positive = df_review[df_review['sentiment']=='positive'][:9000]
df_negative = df_review[df_review['sentiment']=='negative'][:1000]
df_review_imb = pd.concat([df_positive, df_negative])
```

```{r, include = FALSE}
py$df_review_imb %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```
Then, I re-balanced the dataset by randomly sampling my imbalanced samples:

```{python}
from imblearn.under_sampling import  RandomUnderSampler

rus = RandomUnderSampler(random_state=0)
df_review_bal, df_review_bal['sentiment']=rus.fit_resample(df_review_imb[['review']],
                                                           df_review_imb['sentiment'])
df_review_bal
```

```{r, include = FALSE}
py$df_review_bal %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```

I split my newly balanced dataset into training and testing subsets. My training subset was $\frac{2}{3}$ of the balanced dataset and the test subset was the remaining $\frac{1}{3}$.

```{python}
from sklearn.model_selection import train_test_split
train, test = train_test_split(df_review_bal, test_size=0.33, random_state=42)

train_x, train_y = train['review'], train['sentiment']
test_x, test_y = test['review'], test['sentiment']
```

Next, I prepared the data for modelling. To model natural language text predictions, I needed to transform words from the text reviews into a numeric representation of its frequency. I used scikit-learn's term frequency-inverse document frequency (TF-IDF) vectorizer for these frequency calculations. In this approach, a word's TF-IDF score increases as its frequency increases within documents but its score decreases if that word is common among documents.

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')
train_x_vector = tfidf.fit_transform(train_x)

pd.DataFrame.sparse.from_spmatrix(train_x_vector,
                                  index=train_x.index,
                                  columns=tfidf.get_feature_names_out())

test_x_vector = tfidf.transform(test_x)
```

I trained four natural language text analysis models:


1. Support-vector machine learning (SVML) model
2. Classification tree
3. Naive Bayes
4. Logistic regression

```{python, echo = FALSE}
# 1) support-vector machine learning model (SVM)
from sklearn.svm import SVC
svc = SVC(kernel='linear')
svc.fit(train_x_vector, train_y)

# 2) classification tree
from sklearn.tree import DecisionTreeClassifier
dec_tree = DecisionTreeClassifier()
dec_tree.fit(train_x_vector, train_y)

# 3) naive Bayes model
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(train_x_vector.toarray(), train_y)

# 4) logistic regression model
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(train_x_vector, train_y)
```

Finally, I tested that each model generated a prediction given a set of fake reviews. Each model predicts either 'positive' or 'negative' given a string of text:
```{python}
# 1) support-vector machine learning model (SVML)
print(svc.predict(tfidf.transform(['A good movie'])))
print(svc.predict(tfidf.transform(['I did not like this movie at all'])))
```
```{python}
# 2) classification tree
print(dec_tree.predict(tfidf.transform(['A good movie'])))
print(dec_tree.predict(tfidf.transform(['I did not like this movie at all'])))
```
```{python}
# 3) naive Bayes model
print(gnb.predict(tfidf.transform(['A good movie']).toarray()))  # must be transformed: sparse matrix to dense data via x.toarray()
print(gnb.predict(tfidf.transform(['I did not like this movie at all']).toarray()))  # must be transformed: sparse matrix to dense data via x.toarray()
```
```{python}
# 4) logistic regression model
print(log_reg.predict(tfidf.transform(['A good movie'])))
print(log_reg.predict(tfidf.transform(['I did not like this movie at all'])))
```
# Results

Next, I extracted the mean accuracy of each model:
```{python}
svc_score = svc.score(test_x_vector, test_y)
dec_score = dec_tree.score(test_x_vector, test_y)
gnb_score = gnb.score(test_x_vector.toarray(), test_y)
log_reg_score = log_reg.score(test_x_vector, test_y)
```

```{r, echo = FALSE}
scores_df <- data.frame(model = c('Support vector ML', 'Decision tree', 'Naive Bayes', 'Logistic regression'),
                        score = c(py$svc_score, py$dec_score, py$gnb_score, py$log_reg_score))

scores_df %>%
  arrange(desc(score)) %>%
  gt() %>%
  tab_header(
    title = "",
    subtitle = "Table 1. Mean prediction accuracy of four movie review sentiment machine learning models"
  ) %>%
  fmt_number(
    columns = c(score),
    decimals = 2
  ) %>%
  cols_label(
    model = "Model type",
    score = "Mean accuracy"
  ) %>%
  cols_align(
    align = c("center"),
    columns = c(score)
  )
```
Support vector machine learning (SVML) and logistic regression were similarly accurate (`r round(py$svc_score, 2)` and `r round(py$log_reg_score, 2)`, respectively; Table 1). Decision tree and Naive Bayes modelling were considerably less accurate than SVML or logistic regression. Since the SVML model had the highest mean prediction accuracy, I extracted more of its accuracy metrics.

```{python, include = FALSE}
from sklearn.metrics import f1_score
svml_f1_scores = f1_score(test_y, svc.predict(test_x_vector),
                          labels=['positive', 'negative'],
                          average=None)
```

The SVML model had high (i.e., near 1) F1 scores for both positive and negative sentiment reviews, which reinforces this model's prediction accuracy. The SVML model had a positive sentiment F1 score of `r round(py$svml_f1_scores[1], 2)` and a negative sentiment F1 score of `r round(py$svml_f1_scores[2], 2)`.

```{python}
from sklearn.metrics import classification_report
print(classification_report(test_y,
                            svc.predict(test_x_vector),
                            labels=['positive', 'negative']))
```

The confusion matrix

```{python}
# Confusion matrix
from sklearn.metrics import confusion_matrix
conf_mat = confusion_matrix(test_y,
                            svc.predict(test_x_vector),
                            labels=['positive', 'negative'])
```


```{r, echo = FALSE}
intermediate <- 
  as.data.frame(py$conf_mat)%>%
  pivot_longer(c("V1", "V2"))

intermediate$name <- NULL
intermediate$target <- c('negative', 'positive', 'negative', 'positive')
intermediate$prediction <- c('negative', 'negative', 'positive', 'positive')

true_pos <- as.numeric(ifelse(intermediate$target == 'positive' & intermediate$prediction == 'positive', intermediate$value, ""))
true_pos <- subset(true_pos, !is.na(true_pos))
false_pos <- as.numeric(ifelse(intermediate$target == 'positive' & intermediate$prediction == 'negative', intermediate$value, ""))
false_pos <- subset(false_pos, !is.na(false_pos))
true_neg <- as.numeric(ifelse(intermediate$target == 'negative' & intermediate$prediction == 'negative', intermediate$value, ""))
true_neg <- subset(true_neg, !is.na(true_neg))
false_neg <- as.numeric(ifelse(intermediate$target == 'negative' & intermediate$prediction == 'positive', intermediate$value, ""))
false_neg <- subset(false_neg, !is.na(false_neg))
observations <- sum(intermediate$value)
```

A confusion matrix provides context for the SVM model accuracy metrics. In the middle of each tile, we have the normalized count (overall percentage) and, beneath it, the count. At the bottom, we have the column percentage. Of all the observations where the `Target` outcome was `positive`, the SVM model predicted `r sprintf("%0.1f%%", 100*(true_pos/observations))` (`r true_pos` of `r observations`) to be `positive` and `r sprintf("%0.1f%%", 100*(false_pos/observations))` (`r false_pos` of `r observations`) to be `negative`. Of all the observations where the `Target` outcome was `negative`, the SVM model predicted `r sprintf("%0.1f%%", 100*(true_neg/observations))` (`r true_neg` of `r observations`) to be `negative` and `r sprintf("%0.1f%%", 100*(false_neg/observations))` (`r false_neg` of `r observations`) to be `positive`.

```{r, echo = FALSE, warning = FALSE}
plot_confusion_matrix(intermediate, 
                      target_col = "target", 
                      prediction_col = "prediction",
                      counts_col = "value")
```

```{python}
# Model tuning
# maximize the model performance with GridSearchCV
from sklearn.model_selection import GridSearchCV
#set the parameters
parameters = {'C': [1,4,8,16,32] ,'kernel':['linear', 'rbf']}
svc = SVC()
svc_grid = GridSearchCV(svc,parameters, cv=5)

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```



# Discussion




The preF1 score, a value between zero and one, is calculated 
