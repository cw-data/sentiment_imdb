---
title: "Predicting the sentiment of movie reviews with machine learning"
author: "Charles Wainright"
date: "`r format(Sys.Date(), '%m %b, %Y')`"
output: html_document
---
```{r, warning = FALSE, message = FALSE}
library(reticulate)
library(tidyverse)
library(gt)
library(cvms)
set.seed(1)
```

```{python}
import pandas as pd
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
```

# Introduction

This project combines several data science programming languages, principles, and products. I used the R and Python programming languages in this document. I used R (tidyverse, specifically) to summarize and visualize data and model performance. I used Python (scikit-learn, specifically) to create the models and extract their performance metrics. Finally, I used RMarkdown to write this document because it seamlessly interprets code written in multiple programming languages and LaTeX into a single user-friendly document.

# Methods

I created four machine learning models to predict the sentiment (positive or negative) of movie reviews from IMDB. First, I trained the models with a subset of reviews with known review sentiment. Next, I tested model accuracy by predicting review sentiment and comparing determining whether the prediction was correct or incorrect. Then, I selected the most accurate model and tuned its parameters to produce the best possible prediction accuracy.


### Research question
Which machine learning model is most accurate at predicting the reviewer's sentiment given a text review of a movie?

### Model inputs
Text movie reviews.

### Model outputs
Binary predictions: either 'positive' or 'negative'.

### Data

```{python, include = FALSE}
df_review = pd.read_csv('imdb_dataset.csv')
nrows = df_review.shape[0]
ncols = df_review.shape[1]
```

The dataset for this project is from Kaggle and is [available here.](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

The dataset contains `r format(as.numeric(py$nrows), big.mark=",")` rows and `r py$ncols` columns.

```{python}
df_review
```



```{r, include = FALSE}
py$df_review %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```

The dataset started with a balanced number of positive and negative reviews. Since real data may not be perfectly balanced, I imbalanced the samples and then re-balanced them through random re-sampling.

First, I imbalanced the reviews by selecting a different number of positive and negative reviews:

```{python}
df_positive = df_review[df_review['sentiment']=='positive'][:9000]
df_negative = df_review[df_review['sentiment']=='negative'][:1000]
df_review_imb = pd.concat([df_positive, df_negative])
```

```{r, include = FALSE}
py$df_review_imb %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```
Then, I re-balanced the dataset by randomly sampling my imbalanced samples:

```{python}
from imblearn.under_sampling import  RandomUnderSampler

rus = RandomUnderSampler(random_state=0)
df_review_bal, df_review_bal['sentiment']=rus.fit_resample(df_review_imb[['review']],
                                                           df_review_imb['sentiment'])
df_review_bal
```

```{r, include = FALSE}
py$df_review_bal %>%
  group_by(sentiment) %>%
  dplyr::summarize(
    `sample size` = n()
  )
```

I split my newly balanced dataset into training and testing subsets. My training subset was $\frac{2}{3}$ of the balanced dataset and the test subset was the remaining $\frac{1}{3}$.

```{python}
from sklearn.model_selection import train_test_split
train, test = train_test_split(df_review_bal, test_size=0.33, random_state=42)

train_x, train_y = train['review'], train['sentiment']
test_x, test_y = test['review'], test['sentiment']
```

Next, I prepared the data for modelling. To model natural language text predictions, I needed to transform words from the text reviews into a numeric representation of its frequency. I used scikit-learn's term frequency-inverse document frequency (TF-IDF) vectorizer for these frequency calculations. In this approach, a word's TF-IDF score increases as its frequency increases within documents but its score decreases if that word is common among documents.

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')
train_x_vector = tfidf.fit_transform(train_x)

pd.DataFrame.sparse.from_spmatrix(train_x_vector,
                                  index=train_x.index,
                                  columns=tfidf.get_feature_names_out())

test_x_vector = tfidf.transform(test_x)
```

I trained four natural language text analysis models:


1. Support-vector machine learning (SVML) model
2. Classification tree
3. Naive Bayes
4. Logistic regression

```{python, echo = FALSE}
# 1) support-vector machine learning (SVML) model 
from sklearn.svm import SVC
svc = SVC(kernel='linear')
svc.fit(train_x_vector, train_y)

# 2) classification tree
from sklearn.tree import DecisionTreeClassifier
dec_tree = DecisionTreeClassifier()
dec_tree.fit(train_x_vector, train_y)

# 3) naive Bayes model
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(train_x_vector.toarray(), train_y)

# 4) logistic regression model
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(train_x_vector, train_y)
```

Finally, I tested that each model generated a prediction given a set of fake reviews. Each model predicts either 'positive' or 'negative' given a string of text:
```{python}
# 1) support-vector machine learning (SVML) model
print(svc.predict(tfidf.transform(['A good movie'])))
print(svc.predict(tfidf.transform(['I did not like this movie at all'])))
```
```{python}
# 2) classification tree
print(dec_tree.predict(tfidf.transform(['A good movie'])))
print(dec_tree.predict(tfidf.transform(['I did not like this movie at all'])))
```
```{python}
# 3) naive Bayes model
print(gnb.predict(tfidf.transform(['A good movie']).toarray()))  # must be transformed: sparse matrix to dense data via x.toarray()
print(gnb.predict(tfidf.transform(['I did not like this movie at all']).toarray()))  # must be transformed: sparse matrix to dense data via x.toarray()
```
```{python}
# 4) logistic regression model
print(log_reg.predict(tfidf.transform(['A good movie'])))
print(log_reg.predict(tfidf.transform(['I did not like this movie at all'])))
```
# Results


```{python}
svc_score = svc.score(test_x_vector, test_y)
dec_score = dec_tree.score(test_x_vector, test_y)
gnb_score = gnb.score(test_x_vector.toarray(), test_y)
log_reg_score = log_reg.score(test_x_vector, test_y)
```
Support vector machine learning (SVML) and logistic regression were similarly accurate (`r round(py$svc_score, 2)` and `r round(py$log_reg_score, 2)`, respectively; Table 1). Decision tree and Naive Bayes modelling were considerably less accurate than SVML or logistic regression. Since the SVML model had the highest mean prediction accuracy, I extracted more of its accuracy metrics.
```{r, echo = FALSE}
scores_df <- data.frame(model = c('Support vector ML', 'Decision tree', 'Naive Bayes', 'Logistic regression'),
                        score = c(py$svc_score, py$dec_score, py$gnb_score, py$log_reg_score))

scores_df %>%
  arrange(desc(score)) %>%
  gt() %>%
  tab_header(
    title = "",
    subtitle = "Table 1. Mean prediction accuracy of four movie review sentiment machine learning models"
  ) %>%
  fmt_number(
    columns = c(score),
    decimals = 2
  ) %>%
  cols_label(
    model = "Model type",
    score = "Mean accuracy"
  ) %>%
  cols_align(
    align = c("center"),
    columns = c(score)
  )
```


```{python, include = FALSE}
from sklearn.metrics import f1_score
svml_f1_scores = f1_score(test_y, svc.predict(test_x_vector),
                          labels=['positive', 'negative'],
                          average=None)
```

The SVML model had high (i.e., near 1) F1 scores for both positive and negative sentiment reviews, which reinforces this model's prediction accuracy. The SVML model had a positive sentiment F1 score of `r round(py$svml_f1_scores[1], 2)` and a negative sentiment F1 score of `r round(py$svml_f1_scores[2], 2)`.

```{python}
from sklearn.metrics import classification_report
print(classification_report(test_y,
                            svc.predict(test_x_vector),
                            labels=['positive', 'negative']))
```

The confusion matrix

```{python}
# Confusion matrix
from sklearn.metrics import confusion_matrix
conf_mat = confusion_matrix(test_y,
                            svc.predict(test_x_vector),
                            labels=['positive', 'negative'])
```


```{r, echo = FALSE}
intermediate <- 
  as.data.frame(py$conf_mat)%>%
  pivot_longer(c("V1", "V2"))

intermediate$name <- NULL
intermediate$target <- c('negative', 'positive', 'negative', 'positive')
intermediate$prediction <- c('negative', 'negative', 'positive', 'positive')

true_pos <- as.numeric(ifelse(intermediate$target == 'positive' & intermediate$prediction == 'positive', intermediate$value, ""))
true_pos <- subset(true_pos, !is.na(true_pos))
false_pos <- as.numeric(ifelse(intermediate$target == 'positive' & intermediate$prediction == 'negative', intermediate$value, ""))
false_pos <- subset(false_pos, !is.na(false_pos))
true_neg <- as.numeric(ifelse(intermediate$target == 'negative' & intermediate$prediction == 'negative', intermediate$value, ""))
true_neg <- subset(true_neg, !is.na(true_neg))
false_neg <- as.numeric(ifelse(intermediate$target == 'negative' & intermediate$prediction == 'positive', intermediate$value, ""))
false_neg <- subset(false_neg, !is.na(false_neg))
observations <- sum(intermediate$value)
```

A confusion matrix provides context for the SVM model accuracy metrics and provide counts and percentages of true positives, true negatives, false positives, and false negatives. Confusion matrices summarize this context as an overall percentage and count in the center of each tile and row or column percentages on the edge of each tile. Of the `r true_pos + false_pos` observations where the `Target` outcome was `positive`, the SVM model predicted `r true_pos` to be `positive`, yielding a true positive rate of `r sprintf("%0.1f%%", 100*(true_pos/(true_pos + false_pos)))`. The remaining `r false_pos` `positive` predictions were false positives, yielding a false-positive rate of `r sprintf("%0.1f%%", 100*(false_pos/(false_pos + true_pos)))`. Of the `r true_neg + false_neg` observations where the `Target` outcome was `negative`, the SVM model predicted `r true_neg` to be `negative`, yielding a true negative rate of `r sprintf("%0.1f%%", 100*(true_neg/(true_neg + false_neg)))`. The remaining `r false_neg` `negative` predictions were false negatives, yielding a false-negative rate of `r sprintf("%0.1f%%", 100*(false_neg/(true_neg + false_neg)))`.


```{r, echo = FALSE, warning = FALSE}
plot_confusion_matrix(intermediate, 
                      target_col = "target", 
                      prediction_col = "prediction",
                      counts_col = "value")
```

```{python}
# Model tuning
# maximize the model performance with GridSearchCV
from sklearn.model_selection import GridSearchCV
#set the parameters
parameters = {'C': [1,4,8,16,32] ,'kernel':['linear', 'rbf']}
svc = SVC()
svc_grid = GridSearchCV(svc,parameters, cv=5)
svc_grid
svc_grid.fit(train_x_vector, train_y)


print(svc_grid.best_params_)
print(svc_grid.best_estimator_)
```
Parameterization showed that the best `C` parameter was `1` and the best `kernel` parameter was `linear`. Next, I tuned a new SVML model with these parameters:


```{python}
svc2 = SVC(C = 1, kernel='linear')
svc2.fit(train_x_vector, train_y)
```
Then, I tested that the new SVML model would predict given text movie reviews and extracted the mean accuracy for the tuned SVML model.
```{python}
# tuned support-vector machine learning (SVML) model
print(svc2.predict(tfidf.transform(['A good movie'])))
```
```{python}
print(svc2.predict(tfidf.transform(['I did not like this movie at all'])))
```
```{python, echo = FALSE}
# extract mean accuracy for SVML2
svc2_score = svc2.score(test_x_vector, test_y)
```

Mean prediction accuracy was identical for both SVML models.
```{r, echo = FALSE}

scores_df2 <- data.frame(model = c('Un-tuned SVML', 'Tuned SVML'),
                        score = c(py$svc_score, py$svc2_score))

scores_df2 %>%
  arrange(desc(score)) %>%
  gt() %>%
  tab_header(
    title = "",
    subtitle = "Table 2. Mean prediction accuracy of un-tuned and tuned SVML models"
  ) %>%
  fmt_number(
    columns = c(score),
    decimals = 2
  ) %>%
  cols_label(
    model = "Model type",
    score = "Mean accuracy"
  ) %>%
  cols_align(
    align = c("center"),
    columns = c(score)
  )
```


```{python}
# Confusion matrix for SVML2
from sklearn.metrics import confusion_matrix
conf_mat2 = confusion_matrix(test_y,
                            svc2.predict(test_x_vector),
                            labels=['positive', 'negative'])
```


```{r, echo = FALSE, warning = FALSE, message = FALSE}
intermediate2 <- 
  as.data.frame(py$conf_mat2)%>%
  pivot_longer(c("V1", "V2"))

intermediate2$name <- NULL
intermediate2$target <- c('negative', 'positive', 'negative', 'positive')
intermediate2$prediction <- c('negative', 'negative', 'positive', 'positive')

true_pos2 <- as.numeric(ifelse(intermediate2$target == 'positive' & intermediate2$prediction == 'positive', intermediate2$value, ""))
true_pos2 <- subset(true_pos2, !is.na(true_pos2))
false_pos2 <- as.numeric(ifelse(intermediate2$target == 'positive' & intermediate2$prediction == 'negative', intermediate2$value, ""))
false_pos2 <- subset(false_pos2, !is.na(false_pos2))
true_neg2 <- as.numeric(ifelse(intermediate2$target == 'negative' & intermediate2$prediction == 'negative', intermediate2$value, ""))
true_neg2 <- subset(true_neg2, !is.na(true_neg2))
false_neg2 <- as.numeric(ifelse(intermediate2$target == 'negative' & intermediate2$prediction == 'positive', intermediate2$value, ""))
false_neg2 <- subset(false_neg2, !is.na(false_neg2))
observations2 <- sum(intermediate2$value)

plot_confusion_matrix(intermediate2, 
                      target_col = "target", 
                      prediction_col = "prediction",
                      counts_col = "value")
```



The confusion matrix of the tuned model was identical to the un-tuned model, showing that the un-tuned model was no better (or worse) than the tuned model.


# Discussion

I set out to determine which machine learning model was the most accurate at predicting the reviewer's sentiment given a text review of a movie. My analyses showed that two models were similarly accurate at predicting reviewer sentiment: support vector machine learning (SVML) and logistic regression.  
  
  
I investigated my SVML model for optimization because it was slightly more accurate than logistic regression for my dataset. My un-tuned SVML model had a true positive rate of `r sprintf("%0.1f%%", 100*(true_pos2/(true_pos2 + false_pos)))` and a true negative rate of `r sprintf("%0.1f%%", 100*(true_neg/(true_neg + false_neg)))`, which indicated that my TF-IDF vectorization protocol worked well for movie review sentiment predictions. Surprisingly, tuning my SVML model produced no increase in prediction accuracy or improvement in `true positive` or `true negative` rates. Since tuning provided no appreciable benefit, I could investigate combining multiple models for an ensemble of review-sentiment predictions or I could re-visit my logistic regression model to search for better prediction accuracy.
